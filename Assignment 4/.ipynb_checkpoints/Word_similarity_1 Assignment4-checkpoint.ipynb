{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mg/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/mg/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mg/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/mg/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import string\n",
    "from nltk.corpus import wordnet\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_query(query):\n",
    "    '''\n",
    "    This function is for Text normalization\n",
    "    '''\n",
    "    no_punct_query = \"\"\n",
    "    for char in query:\n",
    "        if char not in punctuations:\n",
    "            no_punct_query = no_punct_query + char\n",
    "            \n",
    "    lower_query = no_punct_query.lower()\n",
    "    lower_tokens = lower_query.split(' ')\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in lower_tokens]\n",
    "    stemmed_tokens = [token for token in stemmed_tokens if token not in stopwords.words('english')]\n",
    "    doc = sp(lower_query)\n",
    "    lemm_tokens = [ token.lemma_ for token in doc]\n",
    "    lemm_tokens = [token for token in lemm_tokens if token not in stopwords.words('english')]\n",
    "    \n",
    "    return set(lemm_tokens+stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [\"Sheikh Jarrah is a Palestinian neighborhood in occupied East Jerusalem\",\n",
    "\"Palestinian Jerusalemite families are facing imminent forced eviction from their homes by illegal Israeli settlers\",\n",
    "\"The forced evictions are part of Israel’s systematic policies aimed at erasing the Palestinian identity of Jerusalem\",\n",
    "\"A colossal dam is near completion on Ethiopia’s stretch of the Nile\",\n",
    "\"It is true that Egypt’s survival has hinged on the flow of the Nile’s waters since time immemorial\",\n",
    "\" The filling phase of the colossal dam will, for a time at least, reduce the flow downstream\",\n",
    "\"Arabic  is one of the oldest and most widely spoken languages in the world.\",\n",
    "\"It’s not surprising that the Arabic language has influenced many other languages\",\n",
    "\"Given the extent of the influence of the language, it is still difficult for non-speakers to learn Arabic\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_text = [] \n",
    "for i in data_list:\n",
    "    normalized_text.append(parse_query(i)) # make a list of normalized sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_similarity(sent_1 , sent_2):\n",
    "    mylist = {}\n",
    "    similarity_score=0\n",
    "    for word1 in sent_1:\n",
    "        mylist[word1] = []\n",
    "        for word2 in sent_2:\n",
    "            wordnet_list1 = wordnet.synsets(word1) # get the similiar word from wordnet for word1\n",
    "            wordnet_list2 = wordnet.synsets(word2) # get the similiar word from wordnet for word2\n",
    "            if wordnet_list1 and wordnet_list2: \n",
    "                s = wordnet_list1[0].path_similarity(wordnet_list2[0]) # calculate similarity between the two word sets by using wup_similarity \n",
    "                mylist[word1].append(s)\n",
    "        try:\n",
    "            similarity_score += max(list(filter(None, mylist[word1])))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    similiarity_score_normalized = similarity_score/len(sent_1)\n",
    "    \n",
    "    return similiarity_score_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix =np.zeros((len(normalized_text) , len(normalized_text)))\n",
    "for sent1_num ,sent_1 in enumerate(normalized_text):\n",
    "    for sent2_num ,sent_2 in  enumerate(normalized_text):\n",
    "        if sent1_num == sent2_num:\n",
    "            similarity_matrix[sent1_num][sent2_num] =1 \n",
    "        else:\n",
    "            similarity_matrix[sent1_num][sent2_num]=get_sentence_similarity(sent_1 ,sent_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.39610423, 0.51644737, 0.33958333, 0.36969507,\n",
       "        0.2871732 , 0.38735073, 0.40479323, 0.3712793 ],\n",
       "       [0.22637621, 1.        , 0.34542248, 0.2043674 , 0.20074697,\n",
       "        0.18475891, 0.23912754, 0.26746913, 0.25984386],\n",
       "       [0.2839667 , 0.37730075, 1.        , 0.27891361, 0.30105451,\n",
       "        0.27873601, 0.28059278, 0.3106805 , 0.32595303],\n",
       "       [0.22364302, 0.24177869, 0.26342371, 1.        , 0.34713388,\n",
       "        0.34567303, 0.20878288, 0.21440857, 0.20172401],\n",
       "       [0.19696731, 0.20975131, 0.2561405 , 0.27244126, 1.        ,\n",
       "        0.28101362, 0.19259676, 0.22142425, 0.19650798],\n",
       "       [0.17273243, 0.21618469, 0.29858671, 0.31624906, 0.32871205,\n",
       "        1.        , 0.22269635, 0.21059467, 0.22813853],\n",
       "       [0.22176871, 0.24159412, 0.27155408, 0.19912281, 0.20213834,\n",
       "        0.21238811, 1.        , 0.35034014, 0.35272109],\n",
       "       [0.20421245, 0.25971916, 0.27533688, 0.18624559, 0.22244935,\n",
       "        0.16869549, 0.33296703, 1.        , 0.36923077],\n",
       "       [0.2457483 , 0.3321612 , 0.37755891, 0.25836317, 0.27964675,\n",
       "        0.25777364, 0.41289901, 0.44059324, 1.        ]])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLustering\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=3, random_state=0).fit(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 0, 0, 0, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19778092, 0.22257157, 0.27271697, 0.52956344, 0.55861531,\n",
       "        0.54222888, 0.20802533, 0.21547583, 0.20879017],\n",
       "       [0.22390982, 0.27782483, 0.30814995, 0.21457719, 0.23474481,\n",
       "        0.21295241, 0.58195535, 0.59697779, 0.57398395],\n",
       "       [0.50344764, 0.59113499, 0.62062328, 0.27428811, 0.29049885,\n",
       "        0.25022271, 0.30235702, 0.32764762, 0.3190254 ]])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.cluster_centers_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
